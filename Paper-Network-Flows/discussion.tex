\section{Discussion}\label{sec:discussion}

The previous section demonstrated the efficacy of the \droidxpflow framework for detecting malware in network traffic. In this section, we address the research questions posed in Section~\ref{sec:empirical-study}, presenting the implications of our results, and discussing certain limitations that cannot be ignored. These limitations also highlight areas for future research.

\subsection{Research Questions and Analysis}\label{sec:questions}

The assessment of our method in the previous section allows us to answer the research questions as follows:\newline



\begin{enumerate}
    \item \textbf{Accuracy Gain in Android Malware Classification Using Network Flow Data and ML (RQ1).} Our study indicates that the accuracy of \droidxpflow is competitive when compared to the state-of-the-art in malware classification. In our investigation, the framework achieved an \fone of $0.89$ when using the RF algorithm at ML model.
    
    \item \textbf{Machine Learning Algorithms Analysis (RQ2).} Our experimental findings provide evidence that, among all the ML algorithms investigated, the Random Forest algorithm outperformed the other four algorithms tested. It achieved the highest performance according to the relevant metrics for our dataset (\fds).
    
    \item \textbf{Malware Family Detection Accuracy (RQ3).} The results show that certain malware families significantly benefit from the \droidxpflow framework. For example, samples from the \tjk and \gps families achieved correct classification rates above $93\%$. The main characteristics of these malware families include downloading adware without the userâ€™s knowledge, automatically connecting to and interacting with remote servers, and initiating paid services~\cite{DBLP:journals/jnca/WangCYYPJ19}. Therefore, we can confirm that our framework achieves high accuracy, particularly for malware families that frequently engage in malicious network interactions.
\end{enumerate}

\subsection{Implications}\label{sec:implications}

In this section, we highlight some implications based on the results presented in Section~\ref{sec:results}.\newline

Previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/iceccs/LeB0GL18,DBLP:journals/jss/CostaMMSSBNR22} incorrectly identified the \mas as a solution with a reasonable \fone, based on results from a limited dataset composed of fewer than 20 malware families. In contrast, our analysis revealed negative results for the \mas when using a more representative dataset (LargeDS), which included a greater variety of malware families. These families were responsible for higher false negative rates, ultimately compromising the accuracy of the \mas.

Our work addresses this problem, presenting a approach base on network flow analysis with ML support. Our framework proved to be efficient in detect different malicious behaviors and reduce the number of false negatives. More importantly, the framework can identify more malware families that use polymorphism or obfuscation to evade detection~\cite{DBLP:conf/acsac/MoserKK07}, but exhibit high and suspicious interactions with the network. Malware from the \gps and \dwg families are examples of malicious apps that use these strategies.

Still, our study also reveals that for some malware families, \droidxpflow fails to detect their malicious behavior, while the \mas successfully classified them as malware. This proves that \net is not a complete solution, and highlights the importance of combining both approaches. Among all families explored in $30\%$ of \fds 
 (71 families), 11 families have samples that \droidxpflow do not flagged as malicious, however were classified as malicious by the \mas. Examples include the \fm{Dowgin} family (3 samples out of 69) and the \fm{Revmob} family (4 samples out of 67), where only the \mas was able to identify them as malware. Also, the previews chapter show that \mas is able to correctly label as malware, $100\%$ of samples from \fm{Airpush} family. In this specific case, \droidxpflow also classify as malware all samples and just confirms the maliciousness of them. This demonstrates that the current state-of-the-art Mining Sandbox techniques remain effective for certain malware families.

\subsection{Limitations}\label{sec:limitations}

The previous assessment of results, proved that \droidxpflow is an effective approach for malware detection, however it has some limitations that can not be ignored, and are mentioned below:\newline
\textbf{Training set.} The \fds contains $2,886$ malware samples across $116$ families. When considering the number of apps available on official markets today, we realize that our sample is far from representative. Currently, there are millions of apps on Google Play~\cite{bankmycell}, with a significant number of malicious apps hidden among them. We believe that there are still malware families that cannot be detected by our framework. To address this issue, we propose expanding the training set and testing additional detection models. The malware detection capability improves as the size of the training samples increases, enabling the solution to detect more types of malware. Finally, our work also focuses solely on Android repackaged malware, so we cannot generalize our findings to malware targeting other platforms.\newline
\textbf{Malicious behaviors triggered.} As explained in Section~\ref{sec:data}, during the execution phase of DroidXP it restarts the explored apps to presumably activate the malicious behavior of the malware. However, we are uncertain whether all malicious activities were fully triggered without actual user inputs. Bao et al.~\cite{DBLP:conf/wcre/BaoLL18} provides evidence that DroidBot outperforms other test generation tools by uncovering a larger number of potential malicious behaviors. Nevertheless, we are unsure about its ability to accurately simulate user input, which would make the collected traffic resemble real-world scenarios. Furthermore, since we used a simulated environment, it is possible that some malware could detect this situation, and avoid triggering their malicious behaviors, thus affecting the network traffic collection process. In the future, we plan to explore more recent test generation tools that could cover a wider range of app behaviors. Additionally, we intend to incorporate real devices into the traffic collection process to better detect malware that can bypass environment emulators.