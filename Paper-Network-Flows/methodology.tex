\section{Empirical Assessment}\label{sec:empirical-study}

In this section we present an empirical assessment of\droidxpflow, which extends \droidxp to classify
repackaged apps as malware or non-malware, using network flow data and ML algorithms.
We first characterize the study using the \emph{Goal, Questions, and Metrics} approach (Section~\ref{sec:gqm}),
and then we present the dataset used in our study (Section~\ref{sec:dataset}) and the procedures
we followed for model training (Section~\ref{sec:training})

%\todo[inline]{O objetivo não é esse. O nosso objetivo é compreender
%  como uma abordagem, que explora dados de tráfego de rede, pode melhorar
%  a acurácia do \mas na detecção de malware.}

\subsection{Goal, Questions, and Metrics}\label{sec:gqm}

The {\bf goal} of this empirical study is to understand how effective is \droidxpflow on malware classification.
To achieve this goal, we investigate the following research {\bf questions}:

%\todo[inline]{As questões de pesquisa não estão bem formuladas. Primeiro,
%  questões do tipo 'yes' ou 'no' não nos apoia em um trabalho acadêmico,
%  melhor 'To what extent \ldots'. As outras questões estão genéricas.
%  Temos que estabelecer uma relação com mining sandbox}

\begin{enumerate}[(RQ1)]
\item \rqa
\item \rqb
\item \rqc
% \review{\item \rqe}
\end{enumerate}

We give answers to these questions using standard {\bf metrics} to estimate
model accuracy. To this end, we label the repackaged versions
of the apps in our dataset using the outcomes of \vt---a widely used
infrastructure that relies on a collection of malware engines to keep
record of malicious programs. Using \vt, we compute true positives,
false positives, and false negatives as follows:


\begin{itemize}
\item {\bf True Positive (TP)}. \droidxpflow classifies a repackaged version as a malware and, according to
  \vt, at least two \ses label the asset as a malware. This decision aligns with existing recommendations~\cite{vt-label,DBLP:journals/ese/KhanmohammadiEH19}
   
\item {\bf False Positive (FP)}. \droidxpflow classifies a repackaged version as a malware and, according to \vt, at most one \se labels the asset as a malware.

\item {\bf False Negative (FN)}. \droidxpflow classifies a repackaged version as a malware, and according to \vt, at least two \ses label the asset as a malware.
\end{itemize}

We compute \emph{Precision}, \emph{Recall}, and \emph{F-measure} ($F_1$) from
the number of TP, FP, and FN (using standard formulae). We use basic statistics (average, median, standard deviation) to identify the accuracy of the \ml
for malware classification at each model explored on our dataset.


\subsection{Dataset}\label{sec:dataset}

%\todo[inline]{Precisamos discutir aqui como nós construímos o dataset? Isso não faz parte do artigo anterior?.
%  Podemos publicar esse dataset no Zenodo, ou no IEEE Data (https://ieee-dataport.org/submit-dataset). Podemos revisar
%um pouco aqui \ldots. Um cuidado, repack é um subset de AndroZoo.}

%\todo[inline]{Confuso. Temos 4,076 apps, sendo 1,777 originl and 4,076 repackaged. Frases como essa
%  exigem demais do leitor.}

Our empirical assessment uses the same dataset (\cds) described \pw. This dataset contains 5,844 real-world apps from two
repositories of repackaged Android apps:(\repack~\cite{DBLP:journals/tse/LiBK21} and \amc~\cite{rafiq2022andromalpack}). Of these,
1,777 are original versions and 4,067 are repackaged versions. Multiple repackaged versions of the same original app might
appear within the \cds dataset. According to \vt, 2,886 out of the 4,067 repackaged apps were classified as malware by at least two security engines. That
is, we labeled $70.96$\% of the repackaged apps as malware (and 1,181 of them as non-malware). The \cds dataset contains several
features related to the apps, including information about malware families and a similarity score between the original and repackaged versions of each app.
Further details about the \cds dataset and how they were obtained can be found \pw.

As described in Section~\ref{sec:introduction}, previous studies have shown that $86\%$ of Android malware is found in repackaged
versions of original apps~\cite{DBLP:journals/tdsc/TianYRTP20,DBLP:conf/sp/ZhouJ12}. For this reason, we decided to collect the network traffic
data generated exclusively by the repackaged samples from the \cds, which contains $4,067$ samples ($2,886$ malware and $1,181$ non-malware).
To capture the network traffic data, we execute the repackaged apps in our dataset using the test case generation tool
DroidBot~\cite{DBLP:conf/icse/LiYGC17}, over a period of $3$ minutes.

The outcomes of these executions lead to multiple PCAP files, one for each repackaged app. As mentioned in the previous
section, a PCAP file contains copies of network packets, enabling the analysis of payloads and packet headers~\cite{DBLP:conf/iv/UhlarHR21}.
After that, we use the CICflowMeter~\cite{DBLP:conf/icissp/LashkariDMG17} tool and specific Python scripts to build a
\fds used in this research. \fds contains a total of 1,640 features (see Section~\ref{sec:extraction}), from 4,067 repackaged samples.

\subsection{Model Training}\label{sec:training}

To compare the performance of the ML algorithms, we train the
models based on all 1,640 feature of \fds, and later use it for malware classification. In our learning-based classification procedure,
we split the \fds into a training set consisting of $70\%$
of the samples and a testing set consisting of $30\%$, randomly
selected from the initial \fds. The testing set is used solely
to evaluate the performance of the models (in terms of the metrics
discussed at the begining of this section).
We compare the performance of the following ML algorithms:

%%\rb{Talvez fosse interessante escrever um par\'{a}grafo detalhando a configura\c c\~{a}o desses algoritmos no nosso experimento}

\begin{itemize}
  \item Linear Discriminant Analysis (LDA),
  \item Quadratic Discriminant Analysis (QDA),
  \item Logistic Regression (LR),
  \item Random Forest (RF), and
  \item Energy-based Flow Classifier algorithm (EFC).
\end{itemize}


\rb{Revisar bem o pr\'{o}ximo par\'{a}grafo, talvez incluindo o Prof. Fabiano e o Prof. Eduardo nesse processo.}

To achieve the best-fitting models, we also varied several model parameters using cross-validation~\cite{DBLP:phd/us/Stephenson22} on the training data.
Cross-validation is a technique commonly used in ML to assess how well a model performs on an independent dataset~\cite{DBLP:journals/jsan/AwadF23}.
This technique tests the model on different subsets of the data, helping to detect overfitting and making efficient use of the available data.



%\rb{Não faz sentido apresentarmos a melhor performance do Random Forest aqui, pois estamos antecipando resultados. Talvez comentar todo
%o restante dessa se\c c\~{a}o, ou mover para a proxima se\c c\~{a}o: Results}

%\todo[inline]{N\~{a}o poderia ser usado para justificar as decis\~{o}es da se\c c\~{a}o anterior? Aqui ja fala em Random Forest, mas eh algo que vai ser discutido mais na frente. }





%  %\todo[inline]{Essa se\c c\~{a}o poderia ser chamada de Data Analysis Procedures?}
% \subsection{Model Training and Classifier}\label{sec:learning}

%\todo[inline]{N\~{a}o sei se precisamos introduzir o conceito de ML aqui. }

%\todo[inline]{Aqui deveriamos descrever apenas os m\'{e}todos usados para an\'{a}lise de dados. A menos que se queira deixar claro que Random Forest foi melhor, e os outros n\~{a}o ser\~{a}o mais discutidos no texto. Se for esse o caminho, eh necess\'{a}rio apresentar a performance dos outros aqui. Handrick, eu sugiro fortemente voce ler alguns artigos que usam ML para detectar malwere, e tentar seguir uma estrutura semelhante. Definir ML, Cross-validation, \ldots, aqui n\~{a}o eh bom. Aqui devemos deixar claro nossos metodos de analise de dados. Mas no final, sugiro ler outros artigos e tornar essa secao mais proxima dos outros artigos.}

% To explain our data analysis, we divided the procedure into two steps. First, after selecting the best features for analysis, once again we split the data into a training set consisting of $70$\% of the samples, and a testing set with the remaining $30$\%, randomly selected from \fds. 
% As a second step, we triangulate the results of the \ml classification, with the outputs of \vt, for each model. This may lead to one of the following situations:

% \begin{itemize}
% \item {\bf True Positive (TP)}. The \ml label a repackaged version as a malware and, according to
%   \vt, at least two \ses label the asset as a malware. This decision aligns with existing recommendations~\cite{vt-label,DBLP:journals/ese/KhanmohammadiEH19}
   
% \item {\bf False Positive (FP)}. The \ml label a repackaged version as a malware and, according to \vt, at most one \se labels the asset as a malware.

% \item {\bf False Negative (FN)}. The \ml does not label a repackaged version as a malware, and according to \vt, at least two \ses label the asset as a malware.
% \end{itemize}

% We compute \emph{Precision}, \emph{Recall}, and \emph{F-measure} ($F_1$) from
% the number of TP, FP, and FN (using standard formulae). We use basic statistics (average, median, standard deviation) to identify the accuracy of the \ml for malware classification at each model explored on the \fds dataset.
